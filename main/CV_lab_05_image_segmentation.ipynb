{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNkqD6dOISvSyeNS7PVFaLJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Image Segmentation with `PyTorch`.\n","**üìë Timeline:**\n","1. [What is `Image Segmentation`]().\n","2. [The Terminology and the Notation of `Image Segmentation` explained in depth]().\n","3. [The Methodology of `Image Segmentation`]().\n","4. [`U-Net` Method]().\n","5. [`SegNet` Method]().\n","6. [`SOLO` Method]().\n","7. [`DeepLabv3` Method]().\n","8. [`Mask R-CNN` Method]().\n","9. [A `Semantic Segmentation` Paradigm with `PyTorch`]().\n","10. [Overall Sum Up and further explanation]().\n","\n","---"],"metadata":{"id":"XVnS6BIB3tIo"}},{"cell_type":"markdown","source":["## [1. Image Segmentation with `PyTorch.`]()\n","\n","`Image segmentation` is a technique used in `computer vision` **to divide a digital image into smaller, meaningful parts, or \"`segments`\"** Each `segment` represents a different area or object within the image. The `goal` of this process is to make the image easier to understand and analyze by breaking it down into simpler components.\n","\n","In practical terms, `Œômage Œ£egmentation` works by analyzing the image pixel by pixel. It **groups together** pixels that share certain characteristics, such as color, brightness, or texture, and assigns them a specific label.\n","\n","<img src=\"https://user-images.githubusercontent.com/20102/228140726-a683839d-5038-4961-8f94-5c5a9b3dac2c.png\" alt=\"Example Image\" width=\"600\">\n","\n","For example, in a photo of a dog on a grassy field, all the pixels that make up the dog might be grouped into one `segment` and labeled \"dog,\" while the pixels that form the grass would be grouped into another segment and labeled \"grass.\"\n","\n","This labeling of pixels helps in identifying and isolating specific objects or regions within the image. By segmenting an image in this way, we can focus on and analyze particular parts of the image more effectively, whether for object detection, medical imaging, or any other application where precise identification of image components is necessary.\n","\n","> `Image segmentation` transforms complex images into simpler, labeled regions, making it easier to extract useful information and perform further analysis.\n","\n","#### **Simple Hands-On Explanation.**\n","<img src=\"https://miro.medium.com/v2/resize:fit:1080/1*B16t8Do6hvuq2Q_2YOM-UQ.png\" alt=\"Example Image\" width=\"600\">\n","\n","Imagine you have a photograph of a crowded street. This image contains various objects like cars, pedestrians, buildings, humans and traffic lights. `Image segmentation` helps in breaking down this complex scene into simpler, more understandable components by labeling each pixel according to what object or region it belongs to. For instance, all the pixels that belong to cars might be labeled as \"car,\" while those that belong to pedestrians might be labeled as \"pedestrian.\"\n","\n","This process **involves analyzing the image at the pixel level, grouping similar pixels together based on predefined criteria such as color, texture, or intensity**. By doing this, `segmentation` **creates distinct boundaries within the image, making it easier to identify and analyze different objects or regions.**\n","\n","#### **A Simple Explanation with a `Real-World` Example.**\n","<img src=\"https://es.mathworks.com/help/examples/images_deeplearning/win64/BrainMRISegmentationUsingTrained3DUNetExample_01.png\" alt=\"Example Image\" width=\"600\">\n","\n","Consider the task of medical imaging, such as analyzing an `MRI` scan to detect tumors. *The `MRI` scan is essentially a large image filled with various tissues and structures*. A doctor might need to identify and isolate the tumor from the surrounding tissue. `Image segmentation` **can be used to automatically identify and label the tumor within the `MRI` scan, separating it from other tissues and making it easier for the doctor to focus on the region of interest.**\n","\n","<img src=\"https://www.researchgate.net/publication/366575508/figure/fig9/AS:11431281109338961@1671929191372/Illustrate-segmentation-of-brain-MRI-image-T2-modality-using-active-contour.jpg\" alt=\"Example Image\" width=\"600\">\n","\n","\n","For example, in the `MRI` image, pixels representing the tumor could be assigned a different label than pixels representing healthy tissue. This labeled image can then be used to measure the size of the tumor, track its growth over time, or plan treatment.\n","\n","#### **The Goal of Image Segmentation.**\n","The ***primary goal*** of `image segmentation` **is to transform an image into a form that is easier to understand and analyze**. This is particularly important in fields like medical imaging, autonomous driving, and object detection, where precise identification and localization of objects within an image are critical.\n","\n","By dividing an image into `segments`, `image segmentation` allows for more efficient data analysis and decision-making. For instance, in medical diagnostics, segmented images can help identify abnormalities, while in autonomous vehicles, segmentation can help the car understand its environment by identifying lanes, pedestrians, and other vehicles.\n","\n","### The Classification of `Image Segmentation` Methods.\n","`Image segmentation` methods can be categorized based on the format of the *`resulting segmentation mask`*. These categories include:\n","\n","1. `Semantic Segmentation`.\n","2. `Instance Segmentation`.\n","3. `Panoptic Segmentation`.\n","\n","So, let's explain its one in depth.\n","\n","### 1. `Semantic Segmentation`.\n","\n","<img src=\"https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/614ca8d2d8b99f6c486dbdd7_V7%20dashboard.PNG\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Semantic segmentation` is a type of `image segmentation` where each pixel in an image is classified into a predefined class, without distinguishing between different instances of the same object. All pixels belonging to the same class share the same label.\n","\n","> Simplier, in `semantic segmentation`, the **goal** is to `label` every pixel in the image according to its category. For example, in an image with multiple cars, all pixels belonging to any car would be labeled as \"car,\" without differentiating between individual cars.\n","\n","#### **A Real-World Paradigm.**\n","\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRKcAEX8G7mJJ0EVx4viz8MZM-nBp1Em5EJeQ&s\" alt=\"Example Image\" width=\"600\">\n","\n","\n","In autonomous driving, `semantic segmentation` can be used to identify various objects on the road such as cars, pedestrians, and road signs. The output is a segmented image where each object type (e.g., all cars) is marked with a unique color or label.\n","\n","#### **Key Things.**\n","- `Pixel-wise classification`: Each pixel is classified independently based on its category.\n","- `Class labels`: Objects are grouped into categories like \"car,\" \"person,\" \"road,\" etc.\n","- `No instance differentiation:` All objects of the same class are treated as a single entity.\n","\n","#### **Key Terms.**\n","1. `Segmentation Mask`: An image where each pixel is labeled according to its class.\n","2. `Class Labels`: The categories assigned to different regions in the image.\n","\n","#### **Overview.**\n","`Semantic segmentation` provides a detailed understanding of the image by categorizing every pixel. It is useful for applications where understanding the general layout and composition of the image is more important than identifying individual instances of objects.\n","\n","### 2. `Instance Segmentation`.\n","\n","<img src=\"https://production-media.paperswithcode.com/tasks/instance_seg_example_0xxe9yz.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Instance segmentation` is a more advanced form of `segmentation` **that not only assigns a class label to each pixel but also distinguishes between different instances of the same object**. This means that if there are multiple seeps in an image, each seep will be segmented as a separate entity.\n","\n","> Unlike `semantic segmentation`, `instance segmentation` **recognizes and labels each object instance separately.**\n","\n","#### **Key Things.**\n","- `Instance-aware`: Recognizes and separates different instances of the same class.\n","- `Object detection`: Often combined with object detection to first locate and then segment objects.\n","\n","#### **Key Terms:**\n","1. `Instance Mask`: A `segmentation mask` that identifies each object instance separately.\n","2. `Object Instance`: A unique occurrence of an object in the image.\n","\n","<img src=\"https://www.folio3.ai/blog/wp-content/uploads/2023/05/SS.png\" alt=\"Example Image\" width=\"600\">\n","\n","#### **Overview.**\n","`Instance segmentation` is crucial in scenarios where it‚Äôs important to differentiate between multiple occurrences of the same type of object, such as in object counting or individual object tracking.\n","\n","### 3. `Panoptic Segmentation`.\n","\n","<img src=\"https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/618be38e75ac20a89c315f15_rdvBSvX0Usx45jNMYuqVR9kRHPtEd0sM9xEDu0TNrAaLZs8le5QpDod9rcH_8lrxvYomqW0U7i0YZ7KSwrnedHAvI3nYMvbuQmoo1nIBrMQx4XyM6ZOkq3GchC0IhqDYddE_FncV.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Panoptic segmentation` is a comprehensive approach that **combines both `semantic` and `instance` `segmentation`**. It assigns a class label to each pixel and also differentiates between different instances of objects in the image.\n","\n","> `Panoptic segmentation` provides a complete understanding of the image by combining the benefits of semantic and instance segmentation. It labels each pixel by both its class and instance.\n","\n","#### **A Real-World Paradigm.**\n","\n","<img src=\"https://viso.ai/wp-content/uploads/2024/04/Panoptic-Segmentation-A-Hybrid-Approach-of-Image-Segmentation-Source.jpg\" alt=\"Example Image\" width=\"600\">\n","\n","In complex urban scenes, `panoptic segmentation` **can be used to `segment` and `identify` different types of objects (like cars, pedestrians, trees) while also distinguishing between `multiple instances` of the same type of object**.\n","\n","#### **Key Things.**\n","- `Unified approach`: Merges the objectives of `semantic` and `instance` `segmentation`.\n","- `Complete scene understanding`: Provides detailed information about both object classes and individual instances.\n","\n","#### **Key Terms.**\n","1. `Unified Segmentation Mask`: A mask that provides both class labels and instance differentiation.\n","2. `Scene Understanding`: The ability to interpret the entire scene comprehensively.\n","\n","#### **Overview.**\n","`Panoptic segmentation` **is ideal for tasks requiring detailed scene understanding**, such as in autonomous driving, **where both the `categorization` and `instance identification` of objects are important**.\n","\n","### **Qualitative Comparison.**\n","\n","<img src=\"https://ieg.worldbankgroup.org/sites/default/files/Data/styles/og_image/public/2022-02/fig3_1.png?itok=tuQRzq1-\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Qualitative comparison` in `image segmentation` **involves evaluating the performance** of different `segmentation` methods based on visual inspection and subjective judgment.\n","\n","> By comparing segmented images produced by different methods, we can assess which method produces more accurate, visually appealing, or meaningful results.\n","\n","#### **A `Real-World` Paradigm.**\n","In research, `qualitative comparison` might be used to determine ***which `segmentation algorithm `is best suited for a specific task***, such as medical imaging or satellite imagery analysis.\n","\n","#### **Key Things.**\n","- `Visual Inspection`: The main criterion is how well the `segmentation` aligns with human expectations.\n","- `Subjective Judgment`: Qualitative comparisons rely on human evaluation rather than purely numerical metrics.\n","\n","#### **Key Terms.**\n","1. `Visual Quality`: The clarity and correctness of the `segmented` image.\n","2. `Comparative Analysis`: The process of evaluating and comparing different `segmentation` methods.\n","\n","#### **Overview.**\n","`Qualitative comparison` **is an essential step in evaluating `segmentation` methods**, particularly when selecting the most suitable approach for a specific application or when numerical metrics alone are insufficient.\n","\n","\n","\n","> Note:\n",">\n",">-  You can read more abour Qualitive comorison of differerent segmentation methods [here](https://www.sciencedirect.com/science/article/pii/S0167865597000834).\n","> - Also you can take a look at [this article](https://www.superannotate.com/blog/image-segmentation-for-machine-learning)."],"metadata":{"id":"FXHMAozXKKGQ"}},{"cell_type":"markdown","source":["## [2. The Basic Terminology of `Image Segmentation` explained in depth.]()\n","\n","#### **Basic Terms.**\n","When diving into `image segmentation`, you'll encounter several key terms and concepts. Understanding these will help you grasp how the process works and why it‚Äôs essential. Let's break it down in a more straightforward way:\n","1. `Pixel`.\n","    - `What it is`: Think of a `pixel` as the smallest building block of any digital image. It‚Äôs just a tiny dot of color, and when you put enough of them together, you get the whole picture.\n","    - `Why it matters`: Every action we take in `image segmentation` starts at the `pixel` level. Each `pixel`‚Äôs color and brightness help us decide which part of the image it belongs to.\n","2. `Region`.\n","    - `What it is`: A `region` is a group of `pixels ` that are similar in some way‚Äîmaybe they‚Äôre all the same color or have the same texture. In `segmentation`, we‚Äôre trying to find these regions within an image.\n","    - `Why it matters`: Identifying these `regions` helps us break down the image into more manageable parts, making it easier to analyze or manipulate.\n","3. `Label`.\n","    - `What it is`: A `label` is like a name tag for a `region`. Once we‚Äôve identified a `region`, we assign it a label to say what it is‚Äîlike ‚Äúsky,‚Äù ‚Äútree,‚Äù or ‚Äúcar.‚Äù\n","    - `Why it matters`: By labeling regions, we can start to understand the content of the image and even train computers to recognize similar `regions` in other images.\n","4. `Mask`.\n","    - `What it is`: A `mask` is like a stencil you place over the image to highlight only the parts you‚Äôre interested in. It‚Äôs a black-and-white or colored overlay that tells us which `pixels` belong to which `region`.\n","    - `Why it matters`: `Masks` help us focus on specific areas of the image for detailed analysis or further processing.\n","5. `Boundary`.\n","    - `What it is`: The `boundary` is the `edge` where one `region` ends, and another begins. It‚Äôs the line that separates different objects or areas in the image.\n","    - `Why it matters`: Finding `boundaries` accurately is crucial for making sure we don‚Äôt mix up different regions when labeling them.\n","6. `Superpixel`.\n","    - `What it is`: Instead of dealing with millions of individual `pixels`, sometimes we group them into larger, more meaningful chunks called `superpixels`. It‚Äôs like grouping letters into words instead of reading each letter one by one.\n","    - `Why it matters`: `Superpixels` make the `segmentation` process faster and easier, especially when dealing with large images.\n","7. `Segmantation Map`.\n","    - `What it is`: A `segmentation map` is like a color-coded version of the image where each color represents a different `region` or `label`. It‚Äôs the visual result of the `segmentation` process.\n","    - `Why it matters`: The `segmentation map` helps us see the outcome of our work and understand how well our algorithm is dividing the image into meaningful parts.\n","8. `Ground Truth`.\n","    - `What it is`: `Ground truth` is the ‚Äúcorrect answer‚Äù for `image segmentation`, often created by a human who manually `labels` each `region`. It‚Äôs the standard we compare our results against.\n","    - `Why it matters`: By comparing our `segmentation map` to the `ground truth`, we can measure how accurate our `segmentation `is and see where we need to improve.\n","9. `Over-segmentation vs. Under-segmentation`.\n","    - `Over-segmentation`: This happens when we break the image into too many `regions`, even splitting parts that should stay together.\n","    - `Under-segmentation`: This is the opposite‚Äîwhen we don‚Äôt split the image enough, leaving different objects or regions clumped together.\n","10. `Evaluation Metrics`.\n","    - `Accuracy`: This tells us how often our `segmentation` was correct.\n","    - `Intersection over Union (IoU)`: This metric checks how much the predicted `region` overlaps with the `ground truth`, compared to how much they differ.\n","    - `Dice Coefficient`: Similar to `IoU`, but it‚Äôs a bit more focused on getting the exact overlap right.\n","\n","#### **How It All Comes Together.**\n","When you‚Äôre working on an `image segmentation` project, you start by analyzing each `pixel` to find similar ones and group them into `regions`. You then assign `labels` to these `regions` to identify what they are. You might use a mask to isolate specific areas, and `boundaries` to separate different `regions`.\n","\n","Sometimes, you‚Äôll `group pixels` into `superpixels` to make the process quicker. The final result is a s/`segmentation map` that shows how the image was divided. To see how well your `segmentation` worked, you compare it to the `ground truth` using evaluation metrics like `IoU` or the `Dice coefficient`.\n","\n","By understanding these terms, you‚Äôll be better equipped to tackle `image segmentation tasks`, whether you‚Äôre working on a research project or applying these techniques in real-world applications.\n"],"metadata":{"id":"yuiiLFXZDt8b"}},{"cell_type":"markdown","source":["## [3. The Methodology of `Image Segmentation`.]()\n","\n","### 3. The Methodology of Image Segmentation\n","\n","`Image segmentation` involves various techniques to partition an image into meaningful regions. Two common methods are the ***`Sliding Window`*** method and ***`Fully Connected Convolutional Networks (FCNs)`***.\n","\n","### The `Sliding Window` Method.\n","The `Sliding Window` method is a traditional approach used in image processing **where a fixed-size `window` is moved across the image to perform a local analysis**. At each position, the `window` captures a sub-region (or patch) of the image, and the model classifies whether the center pixel belongs to a particular class or not.\n","\n","#### **Overview and How It Works.**\n","1. `Define the Window Size`: Choose the dimensions of the window (e.g.,`3 x 3`, `5 x 5`). This `window size` should be large enough to capture relevant features but small enough to allow fine-grained analysis.\n","2. `Slide the Window`: The `window `moves across the image, typically with some overlap (`stride`). At each step, the model processes the contents of the `window`.\n","3. `Classify the Center Pixel`: The model uses the information within the w`indow` to classify the center pixel. The process repeats for every possible window position.\n","4. `Create the Segmentation Map`: After processing the entire image, combine the classifications of all center pixels to form the final `segmentation map`.\n","\n","#### **Pros & Cons.**\n","- **`Pros`**:\n","  - Simple to implement and understand.\n","  - Can be used with traditional `ML` models.\n","\n","- **`Cons`**:\n","  - Computationally expensive, especially for large images and small `windows`.\n","  - Ignores global context as it only focuses on local patches.\n","  - Not effective for complex `segmentation` tasks.\n","\n","#### Using Fully Connected Convolutional Networks (FCNs)\n","`Fully Connected Convolutional Networks` (`FCNs`) **are deep learning models designed for `pixel-wise classification tasks`, like `image segmentation`**. Unlike the `sliding window method`, `FCNs` **process the entire image at once and generate a dense output map where each pixel is classified.**\n","\n","##### Overview and How It Works\n","1. `Convolutional Layers`: The `FCN `uses a series of `convolutional layers` to extract hierarchical features from the input image. These layers learn to detect patterns such as edges, textures, and objects.\n","2. `Downsampling (Pooling)`: As the image passes through the network, `downsampling` *(usually through `pooling`)* reduces the `spatial dimensions`. This process captures global context but results in a lower resolution `feature map`.\n","3. `Upsampling (Deconvolution)`: After `downsampling`, the `feature map` is `upsampled` back to the original image size. This `upsampling` is done using techniques like deconvolution or interpolation to create a dense prediction map.\n","4. `Pixel-Wise Classification`: The `upsampled feature map` is passed through a final layer that assigns a class label to each pixel, producing the `segmentation map`.\n","5. `Post-Processing (Optional)`: Sometimes, the `segmentation map` is refined using `post-processing` techniques like Conditional Random Fields (`CRFs)` to smooth the `boundaries` and improve `accuracy`.\n","\n","#### **Pros & Cons.**\n","- **`Pros`**:\n","  - Efficiently processes the entire image, reducing computational overhead.\n","  - Captures both local and global context, improving `segmentation` accuracy.\n","  - Suitable for complex `segmentation tasks` and can be trained `end-to-end`.\n","\n","- **`Cons`**:\n","  - Requires a large amount of labeled data for training.\n","  - High computational cost, especially with large models and high-resolution images.\n","  - Difficult to interpret and debug due to the complexity of deep networks.\n","\n","#### **A small Recap.**\n","- The `Sliding Window` method is straightforward but computationally expensive and less effective for complex tasks. It's primarily used in simpler, traditional models.\n","- `FCNs` are more advanced, leveraging deep learning to provide accurate `segmentation`. They handle entire images in a single pass, making them more efficient and powerful for modern segmentation tasks. However, they require substantial computational resources and large datasets for training."],"metadata":{"id":"ljNeIxnPIst5"}},{"cell_type":"markdown","source":["## [4. `U-Net` Method.]()\n","\n","<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`U-Net` is a type of convolutional neural network (`CNN`) **specifically designed for `image segmentation` tasks**. **It is known for its \"U\" shaped architecture, which consists of a `contracting path` (`encoder`) and an `expansive path` (`decoder`)**. `U-Net` was originally developed for biomedical `image segmentation` but has since been adapted for a wide range of applications.\n","\n","#### **Simple Explanation.**\n","Imagine you have an image, and you want to identify specific regions within it, like finding all the roads on a satellite image or all the cells in a microscopic image. `U-Net` is a tool designed to do exactly that. **It takes an image as input and produces another image where each pixel is labeled according to the object it belongs to.**\n","\n","<img src=\"https://blog.ovhcloud.com/wp-content/uploads/2023/02/Image_segmentation.png\" alt=\"Example Image\" width=\"600\">\n","\n","For instance, if you input a satellite image of a city, U-Net could output a new image where roads are colored one way, buildings another, and green spaces yet another. What makes U-Net special is its ability to understand both the big picture (context) and the fine details, which is crucial for accurate segmentation.\n","\n","#### **The Architecture.**\n","`U-Net‚Äôs` architecture is composed of two main parts: the `contracting path` and the `expanding path`.\n","\n","1. `Contracting path` - (`Encoder`).\n","    - This is the part of the network that reduces the spatial dimensions of the image while increasing the depth (number of feature channels).\n","    - It consists of several `convolutional layers ` followed by `max-pooling` layers.\n","    - The `pooling layers` reduce the image size while preserving the most important features.\n","    - As you go deeper into the network, the spatial information decreases, but the feature representation becomes richer and more abstract.\n","2. `Expanding path`- (`Decoder`).\n","    - The `expanding path` increases the `spatial dimensions` back to the original image size.\n","    - This is done through `upsampling layers` that reverse the effect of the `pooling layers`.\n","    - Importantly, it also uses `skip connections` that directly connect the corresponding layers of the `contracting path` to the `expanding path`.\n","    - These `skip connections` allow the network to combine the `high-resolution` features from the `contracting path` with the `upsampled features`, helping to accurately reconstruct the image's details.\n","\n","> The result is a high-resolution `segmentation map` where each pixel is classified.\n","\n","#### **`Step-by-Step` Process of How `U-Net` Works.**\n","Here‚Äôs how `U-Net` processes an image:\n","Input Image: You start with an image that needs to be segmented.\n","\n","1. `Input Image`: You start with an image that needs to be `segmented`.\n","2. `Encoding` (`Contracting Path`):\n","    - The image passes through several `convolutional layers` that capture increasingly complex features.\n","    - After each `convolution`, a `max-pooling` operation reduces the image size, capturing essential information while discarding unnecessary details.\n","3. `Bottleneck`: The smallest, most abstract representation of the image is created, containing the most critical features in a compact form.\n","4. `Decoding` (`Expanding Path`):\n","    - The abstract features are `upsampled back` to the original image size.\n","    - `Skip connections` from the contracting path are used to combine the upsampled features with the high-resolution features captured earlier.\n","5. `Output Image`:The `final layer` produces a `segmentation map` where each pixel is classified into a specific category.\n","\n","#### **Real World Example of Usage.**\n","\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSy781FUgFGS1tkeVD8RyEBaKXPpwoMlBj7XQ&s\" alt=\"Example Image\" width=\"600\">\n","\n","Consider a medical application where doctors need to segment tumors in `MRI` scans.\n","- `U-Net` can be trained on a set of `labeled MRI` images to learn how to identify tumors.\n","- Once trained, it can automatically process new `MRI scans` and produce a `map` **highlighting the tumor regions**, significantly aiding doctors in diagnosis and treatment planning.\n","\n","\n","#### **How it Works.**\n","`U-Net` **leverages the concept of `convolutional layers` to learn features from the image and `pooling layers` to downsample and focus on the most critical information**. The *architecture‚Äôs key innovation* is the use of `skip connections`, **which help the network retain detailed information even after multiple downsampling operations**. This combination of **global context and fine details** is what makes `U-Net` particularly effective for `segmentation tasks`.\n","\n","#### **Why Choose `U-Net`?**\n","- `Accuracy`: `U-Net`'s architecture ensures that both the context and details of the image are captured, leading to high segmentation accuracy.\n","- `Efficiency`: It‚Äôs designed to work well with relatively small training datasets, making it suitable for specialized applications like medical imaging.\n","- `Versatility`: While originally designed for medical images,`U-Net` has been successfully applied to a wide range of `segmentation` tasks in various fields.\n","\n","#### **When & Where.**\n","`U-Net` is ideal for situations where precise `image segmentation` is crucial, such as:\n","1. `Medical Imaging`: For tasks like `segmenting` tumors, organs, or cells.\n","2. `Satellite Imagery`: For identifying geographical features like rivers, roads, or urban areas.\n","3. `Industrial Applications`: For defect detection or object segmentation in manufacturing processes.\n","\n","> `U-Net` is a powerful and versatile tool for `image segmentation` that combines both deep feature learning and precise spatial localization, making it highly effective for a wide range of tasks."],"metadata":{"id":"aoynz1r4ZDiI"}},{"cell_type":"markdown","source":["## [5. `SegNet` Method.]()\n","\n","<img src=\"https://production-media.paperswithcode.com/methods/segnet_Vorazx7.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`SegNet` **is a `deep learning` architecture specifically designed for `pixel-wise image segmentation`**. It is built on a fully convolutional network (`FCN)` and is optimized for `segmentation tasks` ***where the goal is to `label` every pixel in an image according to the object class it belongs to***. `SegNet` is known for its efficient memory usage and capability to handle large images, making it particularly useful in real-time applications.\n","\n","\n","#### **Simple Explanation.**\n","Imagine you have an image, and you want to identify every pixel's category, such as labeling pixels in a street scene as \"road,\" \"car,\" \"pedestrian,\" etc. `SegNet` is designed to accomplish this task efficiently. It takes an input image, processes it to understand the content, and then produces a segmented output where each pixel is classified according to the object it belongs to.\n","\n","The **key feature** of `SegNet` **is its ability to preserve the spatial details of the image while reducing the computational load**, making it well-suited for applications like autonomous driving, where real-time processing is crucial.\n","\n","\n","#### **The Architecture.**\n","\n","<img src=\"https://www.researchgate.net/publication/358867978/figure/fig3/AS:1149654947373059@1651110507214/The-proposed-SegNet-with-attention-gate-AttSegNet-architecture.png\" alt=\"Example Image\" width=\"600\">\n","\n","`SegNet`'s architecture consists of two main parts: the `encoder` and the `decoder`.\n","\n","1. `Encoder`.\n","    - The `encoder` is a series of `convolutional layers` followed by `pooling layers` (usually `max-pooling`).\n","    - These layers progressively reduce the spatial resolution of the input image while increasing the depth (number of `feature maps`).\n","    - The `encoder` captures the essential features of the image but loses some spatial resolution in the process.\n","2. `Decoder`.\n","    - The `decoder` is designed to upsample the low-resolution `feature maps` back to the original input image size.\n","    - What makes `SegNet` unique is its use of the `max-pooling` indices from the `encoder` to guide the `upsampling` process. This helps to preserve the spatial details and ensures that the segmentation output is accurate.\n","    - The `decoder layers` mirror the `encoder layers` in reverse order, effectively reconstructing the high-resolution `segmentation map`.\n","\n","\n","\n","#### **The `Step-by-Step` Process of How `SegNet` Works.**\n","\n","Here‚Äôs how `SegNet` processes an image `step-by-step`:\n","Input Image: Start with the original image that needs segmentation.\n","\n","1. `Input Image`: Start with the original image that needs segmentation.\n","2. `Encoding` (`Feature Extraction`):\n","    - The image passes through several `convolutional layers` to extract features.\n","    - After each `convolution`, `max-pooling` layers reduce the spatial dimensions, summarizing the important features while discarding less relevant details.\n","    - The indices from the `max-pooling` layers are stored for later use in the `decoding` process.\n","3. `Bottleneck`:At the deepest point of the network, the image is represented as a small, abstract `feature map` containing all the critical information.\n","4. `Decoding` (`Upsampling`):\n","    - The stored `max-pooling indice`s are used to guide the` upsampling` process, ensuring that the spatial structure of the original image is preserved.\n","    - The `decoder` progressively restores the image to its original resolution, with each pixel classified into a specific category.\n","5. `Output Segmentation Map`: The final output is a pixel-wise classification map where each pixel is labeled according to the class it belongs to.\n","\n","#### **Real Worls Example.**\n","One of the primary applications of `SegNet` is in autonomous driving. For instance, an autonomous vehicle needs to understand its surroundings by segmenting the road, pedestrians, cars, and other objects in real-time.` SegNet` can process the vehicle's camera feed to produce a `segmentation ma`p that helps the car \"see\" and understand the environment, making decisions based on this information.\n","\n","#### **How It Works.**\n","`SegNet` uses `CNN` to learn features from an image, and then it uses these features to generate a `segmented output`. The unique aspect of `SegNet` is its `decoding process`, which uses the indices from the `encoder`‚Äôs `pooling layers` to precisely reconstruct the image during upsampling. This allows `SegNet` to produce high-quality `segmentation maps` without needing excessive computational power or memory, making it practical for real-time applications.\n","\n","#### **Why Choose `SegNet`?**\n","- `Efficiency`: `SegNet` is designed to be memory-efficient, making it suitable for real-time applications where resources are limited.\n","- `Accuracy`: The use of `max-pooling i`ndices in the decoder helps maintain spatial details, leading to accurate `segmentation`.\n","- `Versatility`: It performs well across various tasks, from autonomous driving to medical `image segmentation`.\n","\n","#### **When and Where.**\n","\n","`SegNet` is best suited for scenarios where both efficiency and accuracy are critical. Some of these scenarios include:\n","1. `Autonomous Driving`: Real-time `segmentation` of road scenes.\n","2. `Robotics`: Environmental understanding for navigation and interaction.\n","3. `Aerial and Satellite Imagery`: Segmenting landscapes and structures.\n","4. `Medical Imaging`: Identifying and `segmenting` anatomical structures in images.\n","\n","> `SegNet` offers a powerful and efficient solution for `pixel-wise image segmentation`, combining both accuracy and speed, making it an excellent choice for a variety of applications where real-time processing is essential."],"metadata":{"id":"xT4Wf2EFeLjb"}},{"cell_type":"markdown","source":["## [6. `SOLO` Method.]()\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*IAdjAvKFjV-Mck5RMA2_bw.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`SOLO` *(`Segmenting Objects by Locations`)* **is a deep learning method for `instance segmentation` that treats `segmentation` as a `location prediction` problem**. Instead of predicting `masks` or `bounding boxes`, `SOLO` **predicts the location of objects directly in the image**, making it a novel approach to `segmenting` objects individually within a scene.\n","\n","#### **Simple Explanation.**\n","`SOLO` simplifies the complex task of i`nstance segmentation` **by breaking it down into a series of `location-based predictions`**. It assigns a unique location to each object in the image and then uses this information to generate instance masks. The *`key idea`* behind `SOLO` **is that by understanding the spatial location of an object, we can accurately `segment` it without the need for complex `post-processing` steps typically required in other `instance segmentation` methods**.\n","\n","#### **The Architecture.**\n","\n","<img src=\"https://www.researchgate.net/publication/348202825/figure/fig2/AS:985735440134144@1612029051032/The-basic-Segment-Objects-by-LOcations-SOLO-combined-with-You-Only-Look-At-CoefficienTs.png\" alt=\"Example Image\" width=\"600\">\n","\n","`SOLO`'s architecture consists of two main stages:\n","1. `Grid-based Location Prediction`:\n","    - The image is divided into a `grid of cells`, and each cell is responsible for predicting whether an object is present at that location.\n","    - Each cell predicts the `mask` for the object located in its corresponding grid location.\n","\n","2. `Mask Generation`:\n","    - Once the location is identified, the model generates a `binary mask` for the object based on the features extracted from the image.\n","    - The `mask` is refined and aligned with the object‚Äôs boundaries to ensure accurate `segmentation`.\n","\n","#### **The `Step-by-Step` Process of How `SOLO` Works.**\n","\n","1. `Input Image`: The process begins with the input image that needs to be `segmented`.\n","2. `Grid Division`: The image is divided into a `grid of cells`, where each cell is responsible for detecting whether an object exists at its location.\n","3. `Feature Extraction`: A `CNN` is used to `extract features` from the image. These `features` contain essential information about the objects within the scene.\n","4. `Location Prediction`: Each cell in the grid makes a `prediction` about the `presence` of an object and its location within the image.\n","5. `Mask Prediction`:\n","    - For each identified object, a `binary mask` is predicted based on the features extracted from the image.\n","    - The `mask` outlines the object, separating it from the background and other objects.\n","6. `Final Segmentation Map`: The `predicted masks` are combined to form the final s`egmentation map`, where each object is individually segmented with a unique `mask`.\n","\n","#### **Real World Example of Usage.**\n","`SOLO` is particularly useful in scenarios where multiple objects of the same category need to be segmented individually, such as in crowd counting or `segmentation` of similar objects in cluttered scenes. For instance, in a crowded pedestrian scene, SOLO can segment each person individually, which is crucial for tasks like pedestrian detection and tracking.\n","\n","#### **How It Works.**\n","`SOLO` simplifies `instance segmentation` by focusing on predicting the location of objects within a grid, rather than relying on `complex bounding box` or `region proposal networks`. This location-based approach reduces computational complexity and improves the efficiency of the `segmentation` process. The `masks` are generated directly from the predicted locations, ensuring that each object is accurately `segmented` without overlap or confusion.\n","\n","\n","#### **Why Choose `SOLO`?**\n","- `Simplicity`: `SOLO` reduces the complexity of `instance segmentation` by using a `grid-based` approach.\n","- `Efficiency`: It eliminates the need for post-processing steps, making the `segmentation` process faster.\n","- `Accuracy`: By focusing on object locations, `SOLO` can accurately `segment` objects even in crowded scenes.\n","\n","#### **When & Where.**\n","`SOLO` is best suited for scenarios where you need to segment multiple instances of the same category or where objects are closely packed. It‚Äôs particularly effective in:\n","\n","- `Crowd Counting`: `Segmenting` and counting individuals in dense crowds.\n","- `Object Tracking`: Separating and tracking similar objects in cluttered environments.\n","- `Retail`: Segmenting products on shelves for inventory management.\n","\n","> `SOLO` offers a streamlined and effective solution for `instance segmentation`, making it an excellent choice for applications where simplicity, efficiency, and accuracy are critical. Its `location-based` approach provides a new perspective on how to handle `complex segmentation tasks`, particularly in challenging environments with multiple similar objects."],"metadata":{"id":"r0FO5DieqAJX"}},{"cell_type":"markdown","source":["## [7. `DeepLabv3` Method.]()\n","\n","<img src=\"https://cdn.prod.website-files.com/62e939ff79009c74307c8d3e/6454d7183fe5c727c1db5213_763b3a2c.png\" alt=\"Example Image\" width=\"600\">\n","\n","`DeepLabv3` is a ** *state-of-the-art* deep learning method for `semantic image segmentation`**. It is designed to efficiently **capture `multi-scale context` by applying `atrous` (or dilated) `convolutions` in `a spatial pyramid pooling` (`ASPP`) framework.** This allows `DeepLabv3` to `segment image`s with high accuracy while maintaining computational efficiency.\n","\n","#### **Simple Explanation.**\n","\n","<img src=\"https://pytorch.org/assets/images/deeplab2.png\" alt=\"Example Image\" width=\"600\">\n","\n","`DeepLabv3` improves the accuracy of `semantic segmentation` by incorporating multi-scale information through `atrous convolutions`.\n","- `Atrous convolutions` allow the network **to control the resolution of features extracted from the input image, which is crucial for `segmenting objects` at different scales**.\n","- The `ASPP` module further enhances this by `pooling` information from different scales, helping the model to accurately identify and `segment objects` regardless of their size in the image.\n","\n","Imagine you are looking at a landscape with mountains, trees, and rivers. Some features are close to you, while others are far away. `DeepLabv3` **acts like a camera lens that can zoom in and out, capturing details at different distances and ensuring that all parts of the landscape are clearly `segmented`**.\n","\n","#### **The Architecture.**\n","`DeepLabv3`'s architecture consists of several key components:\n","\n","1. `Backbone Network`: Typically a deep `CNN` like `ResNet`, used to extract features from the input image.\n","2. `Atrous Convolutions`: `Atrous convolutions` introduce `gaps` (or `dilations`) between the `convolutional filters`, allowing the network to capture features at various scales without losing resolution.\n","3. `Atrous Spatial Pyramid Pooling` (`ASPP`):\n","    - `ASPP` is a module that applies `atrous convolutions` with different `dilation rates` in parallel. This helps the network gather `contextual information` from multiple scales.\n","    - The `output` from `ASPP` is combined to form a rich `feature representation`, which is then used for `segmentation`.\n","4. `Final Segmentation`: The combined features are passed through a `convolutional layer` to produce the final `segmentation map`, where each pixel is `labeled` with the corresponding class.\n","\n","#### **The `Step-by-step` Process of How `DeepLabv3` Works.**\n","1. `Input Image`: The process begins with an input image that needs to be segmented.\n","2. `Feature Extraction`: A `backbone network` (like `ResNet`) extracts features from the image. These features represent various aspects of the image, such as edges, textures, and object shapes.\n","3. `Atrous Convolution Application`: `Atrous convolutions` are applied to the extracted features at different scales, enabling the network to focus on objects of varying sizes without reducing the feature resolution.\n","4. `ASPP` Module: The `ASPP` module processes the `atrous convolution outputs` by `pooling` information from multiple scales. This enhances the network's ability to capture the global context and recognize objects at different distances and sizes.\n","5. `Feature Fusion`: The outputs from the `ASPP` module are fused together to form a `comprehensive feature map` that contains `multi-scale contextual information`.\n","6. `Segmentation Map Generation`: The `final feature map` is processed through a `convolutional layer`, which generates the s`egmentation map`. Each pixel in this map is assigned a `class label`, indicating the type of object or region it belongs to.\n","\n","#### **Real World Example of Usage.**\n","`DeepLabv3` is widely used in applications like autonomous driving and medical image analysis. For example, in autonomous driving, `DeepLabv3` can `segment` different parts of the road scene, such as cars, pedestrians, road signs, and lanes, helping the vehicle understand and navigate its environment safely.\n","\n","#### **How It Works.**\n","`DeepLabv3` works by combining `atrous convolutions` and `ASPP` to capture rich `contextual information` from an image at multiple scales. This approach allows the model to accurately `segment objects` of varying sizes and in different contexts, leading to high-quality segmentation results. The method is particularly effective for complex scenes where objects are located at different distances or are of different sizes.\n","\n","#### **Why Choose `DeepLabv3`?**\n","- `Multi-Scale Context`: `DeepLabv3` excels at capturing multi-scale information, making it effective for `segmenting objects` of various sizes.\n","- `Accuracy`: The method achieves state-of-the-art performance in `semantic segmentation tasks`.\n","- `Efficiency`: Despite its high accuracy, `DeepLabv3` remains computationally efficient due to its use of atrous `convolutions`.\n","\n","#### **When & Where.**\n","`DeepLabv3` is ideal for tasks that require precise `segmentation` of objects in complex scenes, such as:\n","\n","1. `Autonomous Driving`: `Segmenting` various elements of a road scene for safe navigation.\n","2. `Medical Imaging`: Identifying and `segmenting` different anatomical structures in medical images.\n","3. `Satellite Image Analysis`: `Segmenting` land cover types in `satellite imagery` for environmental monitoring.\n","\n","> `DeepLabv3` is a powerful and efficient method for `semantic segmentation`, leveraging advanced techniques like `atrous convolutions` and `ASPP` to deliver accurate `segmentation` results across a wide range of applications.\n","\n","\n","\n"],"metadata":{"id":"z7qogztKkHgg"}},{"cell_type":"markdown","source":["## [8. `Mask R-CNN` Method.]()\n","\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqhDM_itAJ3fvMx10KU05Y67XroTI3rZO5Qg&s\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Mask R-CNN` **is an advanced deep learning model designed for object detection and instance segmentation**. **It not only identifies and classifies each object in an image but also generates a precise `pixel-level mask` for each detected object**. This makes it a powerful tool for tasks requiring both `detection` and `segmentation`.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png\" alt=\"Example Image\" width=\"600\">\n","\n","\n","Imagine trying to identify and outline every item in a messy room. `Mask R-CNN` not only tells you there‚Äôs a chair, a table, and a lamp, but it also draws the outline around each item, helping you see the exact shape and size of each object.\n","\n","#### **The Architecture.**\n","\n","<img src=\"https://www.researchgate.net/publication/346075907/figure/fig5/AS:960588863778816@1606033640007/Overall-architecture-of-a-Mask-R-CNN.jpg\" alt=\"Example Image\" width=\"600\">\n","\n","\n","`Mask R-CNN` consists of the following key components:\n","\n","1. `Backbone Network`: Typically, a deep `CNN` like `ResNet` is used as the `backbone` to extract `feature maps` from the input image.\n","2. `Region Proposal Network` (`RPN`): This network scans the `feature maps` and proposes regions of interest (`RoIs`) that likely contain objects. These proposals are essentially bounding boxes around potential objects.\n","3. `RoI Align`: Unlike the `RoI pooling` used in `Faster R-CNN`, `Mask R-CNN` introduces `RoI Align`, **which ensures that the features extracted from the proposed regions are better aligned, leading to more accurate predictions.**\n","4. `Bounding Box and Classification Head`: For each `RoI`, a `fully connected network` predicts the object class (e.g., dog, car) and refines the `bounding box`.\n","5. `Mask Prediction Head`: In addition to the `bounding box` and `classification heads`, `Mask R-CNN` includes a `fully convolutional network` that generates a `binary mask` for each `RoI`, indicating the pixels that belong to the object.\n","\n","#### **The `Step-by-Step` Process of How `Mask R-CNN Works`.**\n","1. `Input Image`: Start with an input image that contains multiple objects.\n","2. `Feature Extraction`: A `backbone network `(like `ResNet`) processes the image, producing feature maps that highlight important aspects of the image.\n","3. `Region Proposals`: The `RPN` scans these `feature maps` and proposes regions (`RoIs`) that might contain objects.\n","4. `RoI Align`: Each `RoI` is aligned to a fixed size using `RoI Align`, preserving `spatial information` and improving accuracy.\n","5. `Object Detection`: Each `aligned RoI` is processed through the `bounding box` and `classification head` to predict the `class` of the object and its `bounding box`.\n","6. `Mask Prediction`: For each detected object, the `mask head` generates a `binary mask`, showing the exact pixels that belong to the object within the `bounding box`.\n","7. `Final Output`: The `final output` includes the `bounding box`, `class label`, and `mask` for each detected object, allowing for precise `instance segmentation`.\n","\n","#### **Real World Example of Usage.**\n","`Mask R-CNN` is widely used in applications like autonomous driving and robotics. For instance, in autonomous driving, `Mask R-CNN` can help a car not only detect other vehicles and pedestrians but also understand their precise shapes, which is crucial for safe navigation.\n","\n","#### **How It Works.**\n","`Mask R-CNN` works by integrating `object detection` and `instance segmentation` into a single framework. The `RPN` proposes potential objects, and the network then refines these proposals, classifies the objects, and generates a detailed `segmentation mask` for each one. The addition of `RoI Align` ensures that these predictions are accurate and well-aligned with the original image.\n","\n","#### **Why Choose `Mask R-CNN`?**\n","- `Precision`: `Mask R-CNN` delivers highly accurate `object detection` and `instance segmentation`, making it ideal for applications where precision is critical.\n","- `Versatility`: It can handle multiple objects in a single image, `each with its own unique mask`, making it suitable for complex scenes.\n","- `Improved Accuracy`: `RoI Align` improves the accuracy of both `bounding box` predictions and `segmentation masks`, leading to better overall performance.\n","\n","#### **When & Where.**\n","`Mask R-CNN` is best used in scenarios where precise `object detection` and `segmentation` are needed, such as:\n","1. `Autonomous Driving`: To `detect` and `segment` objects like vehicles, pedestrians, and road signs.\n","2. `Medical Imaging`: For `segmenting` organs or tumors in medical scans.\n","3. `Augmented Reality`: To accurately `segment objects` in real-time for overlaying virtual elements.\n","\n","> `Mask R-CNN` is a powerful method for `object detection` and `instance segmentation`, combining the strengths of `Faster R-CNN` with an additional `mask prediction` branch to deliver precise and detailed `segmentation` results. Its versatility and accuracy make it a top choice for a wide range of real-world applications."],"metadata":{"id":"MCDvkUb9oZNw"}},{"cell_type":"markdown","source":["## [9. Use a `U-Net` for HumanSegmentation - a `PyTorch` Example.]()"],"metadata":{"id":"3y_MF3GJtJfT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlMHCw3Q3gPG"},"outputs":[],"source":["! pip install segmentation-models-pytorch\n","! pip install torchviz\n","! pip install opencv-python"]},{"cell_type":"code","source":["#\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","# The Library that keeps out model architecture and pretrained weights\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.losses import DiceLoss\n","# Smart Augmentation library\n","import albumentations as A\n","\n","# Import necessary libraries\n","import numpy as np\n","import cv2\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"PgSQDJWwcoz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's see what this dataset's repo contains\n","!ls Human-Segmentation-Dataset-master"],"metadata":{"id":"01asns3mWR9y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set Up the configurations\n","root = './'\n","tds_path = './Human-Segmentation-Dataset-master/train.csv'\n","\n","# Set up the cuda device if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define some basic hyperparameters\n","hyps = {\n","    'epochs': 20,            # change the epoch number to play with\n","    'lr': 0.001,\n","    'img_size':320,\n","    'batch_size':32,\n","    'num_classes':1\n","}\n","\n","\n","# Load the data\n","df = pd.read_csv(tds_path)\n","print(df.shape)\n","df.head()"],"metadata":{"id":"klYP-QADaGZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_sample_images (imgs):\n","    '''\n","    Display a batch images\n","    '''\n","    samples = imgs.images           # Get the sample images\n","    _, ax = plt.subplots(1, 5, figsize=(15,3))\n","    ax = ax.flatten()\n","    for i, img in enumerate(samples):\n","        img = cv2.imread(img)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        ax[i].set_title(f'Image {i+1}')\n","        ax[i].imshow(img)\n","\n","\n","def apply_sample_masks(imgs):\n","    '''\n","    Apply the masks to the images\n","    '''\n","    masks = imgs.masks\n","    _, ax = plt.subplots(1, 5, figsize=(15,3))\n","    ax = ax.flatten()\n","    for i, mask in enumerate(masks):\n","        mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE) / 255.0\n","        ax[i].set_title(f'Ground Truth {i+1}')\n","        ax[i].imshow(mask, cmap='gray')\n"],"metadata":{"id":"P5O2GQHDWe9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explore the data and the segment mask transformations\n","sample_batch = df.iloc[np.random.randint(0, df.shape[0], size=5)]\n","display_sample_images(sample_batch)\n","apply_sample_masks(sample_batch)"],"metadata":{"id":"YpuLEX0bZatb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data to training and validation set easily with only one function\n","train_set, val_set = train_test_split(df, test_size=0.2, random_state=57)"],"metadata":{"id":"c3b6VLQrl3dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Augmentations:\n","    '''\n","    The training and evaluation\n","    data transformations\n","    '''\n","    def __init__(self, img_size, prob=0.5):\n","        self.img_size = img_size\n","        self.prob = prob\n","\n","    def get_train_transforms (self):\n","        return A.Compose([\n","            A.Resize(self.img_size, self.img_size),\n","            A.HorizontalFlip(p=self.prob),\n","            A.VerticalFlip(p=self.prob),\n","           # A.RandomRotate90(p=self.prob),\n","        ], is_check_shapes=False)\n","\n","    def get_val_transforms (self):\n","        return A.Compose([\n","            A.Resize(self.img_size, self.img_size),\n","        ], is_check_shapes=False)"],"metadata":{"id":"gboaNgKSyMZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a Custom Datase\n","class HumanSegmentationDataset(Dataset):\n","    def __init__(self, df, augmentations):\n","        self.df = df\n","        self.augmentations = augmentations\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # Get the images and masks\n","        samples = self.df.iloc[idx]\n","        img = samples.images\n","        mask = samples.masks\n","\n","        # Read images and masks\n","        img = cv2.imread(img)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n","        mask = np.expand_dims(mask, axis=-1)\n","\n","        # Augmentations\n","        if self.augmentations:\n","            data = self.augmentations(image=img, mask=mask)\n","            img = data['image']\n","            mask = data['mask']\n","\n","        # Transpose image dimensions in pytorch format\n","        # aka: (H,W,C) -> (C,H,W)\n","        img = np.transpose(img, (2,0,1)).astype(np.float32)\n","        mask = np.transpose(mask, (2,0,1)).astype(np.float32)\n","\n","        # Normalize the images and masks\n","        img = torch.Tensor(img) / 255.0\n","        mask = torch.round(torch.Tensor(mask) / 255.0)\n","\n","        return img, mask"],"metadata":{"id":"PF4JyxUqykNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = HumanSegmentationDataset(train_set, Augmentations(hyps['img_size']).get_train_transforms())\n","val_dataset = HumanSegmentationDataset(val_set, Augmentations(hyps['img_size']).get_val_transforms())\n","\n","print(f'Train Dataset Size: {len(train_dataset)}')\n","print(f'Validation Dataset Size: {len(val_dataset)}')"],"metadata":{"id":"05lJ7eiIywPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets apply mask to an image and create a pair\n","def pair_img_mask (idx):\n","    img , mask = train_dataset[idx]\n","\n","    # Show the natural image\n","    plt.subplot(1,2,1)\n","    plt.imshow(np.transpose(img, (1,2,0)))\n","    plt.axis('off')\n","    plt.title('Image')\n","\n","    # Show the masked image\n","    plt.subplot(1,2,2)\n","    plt.imshow(np.transpose(mask, (1,2,0)), cmap='gray')\n","    plt.axis('off')\n","    plt.title(\"GROUND TRUTH\");\n","    plt.show()"],"metadata":{"id":"6a4ycKeoyypY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in np.random.randint(0, len(train_dataset), 2):\n","    pair_img_mask(i)"],"metadata":{"id":"d8SEoeppyzkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=hyps['batch_size'], shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=hyps['batch_size'], shuffle=True)\n","\n","print(f\"Total number of batches in Train Loader: {len(train_loader)}\")\n","print(f\"Total number of batches in Val Loader: {len(val_loader)}\")\n","\n","\n","# Display the sizes\n","for img, mask in train_loader:\n","    print(f\"Size of one batch of images: {img.shape}\")\n","    print(f\"Size of one batch of masks: {mask.shape}\")\n","    break"],"metadata":{"id":"x_01D6uay8pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the Segmentation Model\n","class SegmentationModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SegmentationModel, self).__init__()\n","\n","        self.model = smp.Unet(\n","            encoder_name='resnet50',\n","            encoder_weights='imagenet',\n","            in_channels=3,\n","            classes=num_classes,\n","            activation=None\n","        )\n","\n","    def forward(self, imgs, masks=None):\n","        logits = self.model(imgs)\n","\n","        if masks != None:\n","            loss1 = DiceLoss(mode='binary')(logits, masks)\n","            loss2 = nn.BCEWithLogitsLoss()(logits, masks)\n","            return logits, loss1 + loss2\n","        return logits"],"metadata":{"id":"5-GEkN9-y9jC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instatiate model and set it to device\n","model = SegmentationModel(hyps['num_classes']).to(device)"],"metadata":{"id":"YvjT_rw2zCLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the training and validation routines\n","def train_loop (data_loader, model, optimizer, device):\n","    total_loss = 0.0\n","    model.train()\n","\n","    for imgs, masks in tqdm(data_loader):\n","        imgs = imgs.to(device)\n","        masks = masks.to(device)\n","\n","        optimizer.zero_grad()\n","        logits, loss = model(imgs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(data_loader)\n","\n","def val_loop (data_loader, model, device):\n","    total_loss = 0.0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for imgs, masks in tqdm(data_loader):\n","            imgs = imgs.to(device)\n","            masks = masks.to(device)\n","\n","            logits, loss = model(imgs, masks)\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)"],"metadata":{"id":"T68fOvONzE5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=hyps['lr'])\n","best_val_loss = 1e9\n","\n","# Store the metrics values for plotting\n","tr_epoch_loss, vl_epoch_loss = [] , []\n","\n","# Train the model\n","for epoch in range(hyps['epochs']):\n","    train_loss = train_loop(train_loader, model, optimizer, device)\n","    val_loss = val_loop(val_loader, model, device)\n","\n","    if val_loss < best_val_loss:\n","        # Update the new best validation\n","        best_val_loss = val_loss\n","        # Save the new best model\n","        torch.save(model.state_dict(), 'best_model.pth')\n","        print('Model saved!')\n","\n","    # Append the losses\n","    tr_epoch_loss.append(train_loss)\n","    vl_epoch_loss.append(val_loss)\n","\n","    print(f'Epoch {epoch+1}/{hyps[\"epochs\"]}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","# Plot Loss\n","plt.plot(tr_epoch_loss, label='Training Loss')\n","plt.plot(vl_epoch_loss,label='Validation Loss')\n","plt.legend()\n","plt.show"],"metadata":{"id":"2yyj5Sf7zHTy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After all of this steps the `U-Net` has been trained! Good job, it wasn't so dummy dificult am I right? ü§™!\n","Now let's see how our model predictions look in comparison to the original masks."],"metadata":{"id":"W3j_7Fj2zMwT"}},{"cell_type":"code","source":["# Load the best model\n","model.load_state_dict(torch.load('best_model.pth'))\n","\n","def prediction_mask (idx):\n","    ''' output the prediction mask\n","    '''\n","    img, mask = val_dataset[idx]\n","    logits_mask = model(img.to(device).unsqueeze(0))  # it is just the conversion (C, H, W) -> (1, C, H, W)\n","\n","    pred_mask = torch.sigmoid(logits_mask)\n","    pred_mask = (pred_mask > 0.5) * 1.0\n","\n","    return img, mask, pred_mask"],"metadata":{"id":"Bk0Z1s6mzKhT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how good is actually the prediction."],"metadata":{"id":"0yRv6pbPzW82"}},{"cell_type":"code","source":["# Compare predictions with original\n","for i in np.random.randint(0, len(val_set), 5):\n","    img, mask, pred_mask = prediction_mask(i)\n","\n","    # Show image\n","    plt.figure(figsize=(10,3))\n","    plt.subplot(1,3,1)\n","    plt.imshow(np.transpose(img, (1,2,0)))\n","    plt.axis('off')\n","    plt.title(f'Image {i+1}');\n","\n","    # Show original mask\n","    plt.subplot(1,3,2)\n","    plt.imshow(np.transpose(mask, (1,2,0)), cmap='gray')\n","    plt.axis('off')\n","    plt.title(f'Ground Truth {i+1}');\n","\n","    # Show predicted mask\n","    plt.subplot(1,3,3)\n","    plt.imshow(np.transpose(pred_mask.detach().cpu().squeeze(0), (1,2,0)), cmap='gray')\n","    plt.axis('off')\n","    plt.title(f'Prediction {i+1}');"],"metadata":{"id":"Ijfd6ImMzSpP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [10. Overall Sum Up and further explanation.]()"],"metadata":{"id":"vLs3i2lQtYZ9"}},{"cell_type":"markdown","source":["In this tutorial, we've explored various image segmentation methods, each with its own strengths and applications. Let‚Äôs wrap things up and highlight the key takeaways:\n","\n","#### üöÄ **Key Methods Covered.**\n","1. **`U-Net`**: ü©∫ A robust model for tasks like biomedical `image segmentation`, known for its *U-shaped* architecture with `skip connections` that preserve spatial details.\n","\n","2. **`SegNet`**:‚ö°Ô∏è Designed for efficient `segmentation` in real-time applications. It uses a *symmetric* `encoder`-`decoder` structure, ideal for scenarios where speed matters, such as autonomous driving üöó.\n","\n","3. **`SOLO`**: üéØ A novel approach that focuses on `location-based classification`, making it highly effective for `instance segmentation`.\n","\n","4. **`DeepLabv3`**: üîç Enhances `segmentation` by using dilated `convolutions` and `atrous spatial pyramid pooling` (`ASPP`), allowing for a broader context while keeping details sharp.\n","\n","5. **`Mask R-CNN`**:üõ°Ô∏è A comprehensive model for both `object detection` and `instance segmentation`, adding a `mask prediction` branch to the `Faster R-CNN` framework for pixel-level precision.\n","\n","#### üß† **Takeaways.**\n","\n","- **`Precision in Segmentation`**: üéØ Each method is designed to achieve a certain level of detail.` Mask R-CNN`, for example, excels at `instance segmentation ` with detailed `masks`, while `U-Net` is perfect for tasks needing pixel-level accuracy, like medical imaging ü©ª.\n","\n","- **`Architecture is Key`**: üèóÔ∏è The success of these models hinges on their architecture. `U-Net`‚Äôs `skip connections` help in handling complex tasks with high accuracy, while `DeepLabv3`‚Äôs `atrous convolutions` capture fine details over large areas üåç.\n","\n","- **`Real-World Uses`**: üåê `U-Net` is widely used in medical imaging, `Mask R-CNN` in self-driving cars üöó and AR, and `SegNet` in systems where speed is essential üèéÔ∏è.\n","\n","- **`Balancing Accuracy and Speed`**: ‚öñÔ∏è Sometimes, you need to trade-off between accuracy and efficiency. `SegNet` is great for real-time tasks, while `Mask R-CNN` offers higher accuracy at the cost of being more resource-intensive üß†.\n","\n","#### üîç **Further Insights.**\n","- **`Choosing the Right Method`**: üß© Pick the method based on your needs. `Mask R-CNN` is best for complex, detailed tasks, while `SegNet` is ideal for fast, real-time applications ‚ö°Ô∏è.\n","- **`Importance of Data`**: üìä The success of these models depends heavily on data quality. Preprocessing steps like `normalization` and `augmentation` are crucial for getting the best results üßº.\n","- **`Looking Ahead`**: üëÄ `Image segmentation` is evolving quickly, with new models, like those based on transformers, showing promise. Keep experimenting and exploring to find the best solutions üõ†Ô∏è.\n","\n","### üí°Final Thoughts.\n","`Image segmentation` is a powerful tool that turns raw image data into meaningful insights. Each method we‚Äôve covered offers something unique, and with the right approach, you can achieve impressive results ü•á. Keep experimenting, and don‚Äôt be afraid to try out different methods to see what works best for your project. Happy coding! ü§óü™õüíª"],"metadata":{"id":"aQ3-A118uP8P"}}]}