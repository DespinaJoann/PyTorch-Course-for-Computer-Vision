{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMpXHi8pJhwfEOyQ+gfQGEB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d36825052cba4c829c91ca2cd25e3b95":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e10d696848e40fab4d2b2da486dbc0f","IPY_MODEL_b049d886c8d74c638d3dc5a014738979","IPY_MODEL_a816c30de54049968538b26f34647a27"],"layout":"IPY_MODEL_3b95fca329454d9fb5c64027c11ead14"}},"9e10d696848e40fab4d2b2da486dbc0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b817869598ef41bd9b5e7898aaa3dfd1","placeholder":"​","style":"IPY_MODEL_c4635ac7070e45f296407c79aba89ace","value":"Map: 100%"}},"b049d886c8d74c638d3dc5a014738979":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_953132923cef4b80af41c3b947e12f31","max":1020,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48f32fff05e14880a1218ebcc75864a7","value":1020}},"a816c30de54049968538b26f34647a27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5c30a94d50b44c2a71340fb882ca1a1","placeholder":"​","style":"IPY_MODEL_94bac8c5d8ed40df86614012a10ebaf4","value":" 1020/1020 [01:12&lt;00:00, 14.16 examples/s]"}},"3b95fca329454d9fb5c64027c11ead14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b817869598ef41bd9b5e7898aaa3dfd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4635ac7070e45f296407c79aba89ace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"953132923cef4b80af41c3b947e12f31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48f32fff05e14880a1218ebcc75864a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5c30a94d50b44c2a71340fb882ca1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94bac8c5d8ed40df86614012a10ebaf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"424fecd0cbe24c8aab1f0acbf1fedabb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21da699d2cbe4175be34a0a9662d3dd6","IPY_MODEL_912f1b0d2d804295a73631ea289d4dac","IPY_MODEL_38a9c1655e144f15aa826b9b1a182d9a"],"layout":"IPY_MODEL_366e035b7af744828df08f7c18e08105"}},"21da699d2cbe4175be34a0a9662d3dd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eab82023b86a4388ad6405786346bd6b","placeholder":"​","style":"IPY_MODEL_f18d02d354254d2c88753c036c66a361","value":"Map: 100%"}},"912f1b0d2d804295a73631ea289d4dac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37ff5f9c9eb84b149a0c5de64b3f7b8b","max":1020,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99bb2441a92b40c682100dc40dba020a","value":1020}},"38a9c1655e144f15aa826b9b1a182d9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30b844f160d849599d1b706ac80478d3","placeholder":"​","style":"IPY_MODEL_99a511ba608f4146aa36f24cfc101bbe","value":" 1020/1020 [00:47&lt;00:00, 21.39 examples/s]"}},"366e035b7af744828df08f7c18e08105":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab82023b86a4388ad6405786346bd6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f18d02d354254d2c88753c036c66a361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37ff5f9c9eb84b149a0c5de64b3f7b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99bb2441a92b40c682100dc40dba020a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30b844f160d849599d1b706ac80478d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99a511ba608f4146aa36f24cfc101bbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Hugging Face 🤗 and Vision Transformers for Image Classification**\n","\n","\n","---\n"],"metadata":{"id":"NLTR76kyrWmn"}},{"cell_type":"markdown","source":["\n","# **A Brief Introduction to Hugging Face 🤗**\n","\n","Hugging Face is an AI and machine learning platform that provides easy-to-use tools, libraries, and a community-driven repository for sharing models, datasets, and code. While Hugging Face is best known for its contributions to natural language processing (NLP), it also supports a wide range of tasks, including computer vision and audio processing.\n","\n","#### **How Hugging Face Works**\n","\n","Hugging Face focuses on three main areas:\n","\n","- **Model Hub**: This is a central repository that hosts thousands of pre-trained models for different tasks such as NLP, vision, and audio. You can easily load these models and fine-tune them for your specific needs.\n","  \n","- **Transformers Library**: This library offers APIs to load, train, and fine-tune a variety of popular machine learning models, including architectures like BERT, GPT, and Vision Transformers (ViT).\n","\n","- **Datasets Library**: Hugging Face provides access to a wide range of curated datasets, simplifying the process of loading and preparing data for machine learning tasks.\n","\n","> #### **Quick Start with Hugging Face**\n",">\n","> Hugging Face makes it incredibly easy to start experimenting with state-of-the-art models. Here’s how:\n","> 1. **Select a pre-trained model** from the Hugging Face Model Hub.\n","> 2. **Load the model** along with its tokenizer or feature extractor.\n","> 3. **Apply the model** to tasks such as classification, text generation, or image recognition.\n","> 4. **Fine-tune or deploy** the model to customize it for your specific needs.\n","> *In just a few simple steps, you're ready to explore and experiment with cutting-edge AI models!*\n","\n","#### **Why Use Hugging Face?**\n","\n","Here are some reasons why Hugging Face is an excellent choice for machine learning:\n","\n","- **Access to pre-trained models**: Hugging Face offers thousands of pre-trained models, which can save you significant time and computing resources when starting new projects.\n","  \n","- **Easy model sharing**: The platform allows researchers and developers to easily share their models and datasets, or use those created by others.\n","\n","- **Support for a wide range of tasks**: Hugging Face models cover tasks like NLP, vision, and audio, allowing you to work across multiple domains with ease.\n","\n","- **Preprocessing tools**: The Hugging Face `transformers` and `datasets` libraries include built-in tools for tokenization, feature extraction, and data preparation, making the development process smoother.\n","\n","- **Strong community and support**: Hugging Face has an active community, extensive documentation, and plenty of examples to help you get started quickly.\n","\n","#### **Key Takeaways Before Moving to the Code**\n","\n","1. **Pre-trained models**: Hugging Face offers models trained on large datasets (e.g., BERT, GPT, ViT), which can be fine-tuned for your specific tasks.\n","  \n","2. **Model Hub**: The Model Hub is organized by tasks like NLP and computer vision, and each model comes with detailed documentation on how to use it.\n","\n","3. **Transformers Library**: This is the core of Hugging Face. It enables easy access to models for text generation, classification, translation, and tasks like image classification using Vision Transformers (ViT).\n","\n","4. **Training and fine-tuning**: Hugging Face provides tools like the `Trainer` API, which simplifies the process of training and fine-tuning models on custom datasets.\n","\n","---"],"metadata":{"id":"T-68n8GWt7l5"}},{"cell_type":"markdown","source":["# **Creating an Account on Hugging Face and Setting It Up in Google Colab.**\n","\n","Now, let’s prepare the environment so that we’re fully equipped to dive into the tutorial. By following the steps outlined below, we will ensure that all the necessary libraries and configurations are in place. This setup will allow you to seamlessly follow along and make the most out of the Hugging Face tools and resources we’ll be using. Getting everything ready now will save time later and help you focus on learning and experimenting with cutting-edge AI models!\n","\n","#### **Step 1: Create a Hugging Face Account.**\n","1. **Visit Hugging Face**: Go to the [Hugging Face](https://huggingface.co/) website.\n","2. **Sign Up**:\n","    - Click on the Sign Up button in the top right corner.\n","    - Fill in your email, username, and create a password.\n","    - Alternatively, you can sign up using your GitHub or Google account for quicker access.\n","3. **Verify Your Email**: After registering, you will receive a verification email. Click the link in the email to activate your Hugging Face account.\n","\n","#### **Step 2: Generate an `API` Token on Hugging Face**\n","To use Hugging Face in Google Colab, you will need to generate an API token.\n","1. **Go to Your Profile**: Once logged in, click on your profile picture (top right) and select Settings.\n","2. **Access `API` Tokens**: In the left menu, click on Access Tokens.\n","3. **Generate a New Token**:\n","    - Click on the New Token button.\n","    - Give it a name, like \"Google Colab\".\n","    - Set the permissions to Read if you only need to access data, or Write if you want to upload models/datasets.\n","    - Click Generate and copy the token.\n","\n","\n","#### **Step 3: Set Up Hugging Face in Google Colab.**\n","Now, let's set up the environment to be ready for the tutorial.\n","1. **Open Google Colab**: Go to [Google Colab](https://colab.research.google.com/) and create a new notebook.\n","2. **Install the `transformers` Library**: Run the following command in a new cell to install the Hugging Face libraries:"],"metadata":{"id":"i-rTfm180gjQ"}},{"cell_type":"code","source":["!pip install transformers datasets"],"metadata":{"id":"auR7F-wXS9y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729621177112,"user_tz":-180,"elapsed":4019,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"cd25bd06-6a16-4257-f806-8599aaeeef4f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"]}]},{"cell_type":"markdown","source":["\n","3. **Log in to Hugging Face in Colab**: Use your API token to log in:\n"],"metadata":{"id":"i3NsIZLpTG1K"}},{"cell_type":"code","source":["from huggingface_hub import login\n","# login(token=\"your_api_token_here\")"],"metadata":{"id":"io5yfJeaS-oa","executionInfo":{"status":"ok","timestamp":1729621177113,"user_tz":-180,"elapsed":17,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["4. **Test the Setup**: You can now load and use pre-trained models from Hugging Face. For example:\n"],"metadata":{"id":"M5RSYe-JTH1O"}},{"cell_type":"code","source":["from transformers import pipeline\n","# Load a sentiment-analysis pipeline\n","classifier = pipeline(\"sentiment-analysis\")\n","# Test with sample text\n","result = classifier(\"Hugging Face is amazing!\")\n","print(result)"],"metadata":{"id":"Kv5S82GlTCHC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729621178145,"user_tz":-180,"elapsed":1047,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"09d169f1-daa2-4b39-fe12-f17604d5ee2b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]},{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9998840093612671}]\n"]}]},{"cell_type":"markdown","source":["\n","   This will output the sentiment classification for the input text.\n","\n","#### **You're All Set!**\n","\n","Now you can start using Hugging Face models and datasets directly from Google Colab! Whether you're working on natural language processing, computer vision, or audio tasks, Hugging Face makes it easy to experiment with cutting-edge AI models quickly and efficiently.\n","\n","---\n","\n"],"metadata":{"id":"K95MRKBBTqOf"}},{"cell_type":"markdown","source":["\n","# **Hugging Face with Vision Transformers**\n","\n","In this section, we will explore how to leverage Hugging Face’s capabilities with Vision Transformers (ViTs). These models have revolutionized computer vision tasks, and Hugging Face provides an easy-to-use interface for accessing and deploying them.\n","\n","#### **1. The Library with Pre-trained Models.**\n","Hugging Face’s `transformers` library includes a variety of pre-trained Vision Transformer models. These models have been trained on large datasets, making them effective for various computer vision tasks such as image classification, object detection, and segmentation.\n","\n","#### **2. How to Load the Model.**\n","To load a Vision Transformer model in Hugging Face, you can use the following code snippet:\n","\n","```python\n","from transformers import AutoModelForImageClassification, AutoTokenizer\n","\n","# Load the model and tokenizer\n","model_name = \"google/vit-base-patch16-224-in21k\"  # Example model\n","model = AutoModelForImageClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","```\n","\n","This code allows you to easily access pre-trained weights and configuration for the selected model.\n","\n","#### **3. Built-in Functions and Automations Explained.**\n","Hugging Face provides a range of built-in functions that simplify common tasks. For example:\n","- **Preprocessing**: The library includes functions for resizing, normalizing, and augmenting images.\n","- **Inference**: Use the `pipeline` API to quickly run inference on images without needing to set up complex workflows.\n","- **Evaluation Metrics**: The library provides easy access to standard metrics for evaluating model performance, such as accuracy and F1 score.\n","\n","#### **4. Why Use Hugging Face's Automations Instead of Your Own Code?**\n","Using Hugging Face's built-in functions and automations can save you significant time and effort. Here are a few reasons to consider:\n","- **Simplicity**: Hugging Face abstracts away the complexity, allowing you to focus on your tasks rather than the underlying implementation.\n","- **Community Support**: The library is continuously updated by the community, providing bug fixes and new features regularly.\n","- **Standardization**: By using established functions, you ensure consistency in your code and results, making it easier to share and collaborate with others.\n","\n","#### **5. How to Create a Supported Dataset for Hugging Face.**\n","Creating a dataset that is compatible with Hugging Face involves the following steps:\n","1. **Collect and Organize Your Data**: Ensure your images are stored in a structured format, such as separate folders for different classes.\n","2. **Use the `datasets` Library**: Leverage Hugging Face’s `datasets` library to load and preprocess your images. For example:\n","   ```python\n","   from datasets import load_dataset\n","   dataset = load_dataset(\"imagefolder\", data_dir=\"path_to_your_dataset\")\n","   ```\n","\n","3. **Preprocess the Images**: Apply any necessary transformations (e.g., resizing, normalization) to prepare your dataset for training or evaluation.\n","4. **Split the Dataset**: Divide your dataset into training, validation, and test sets to ensure effective model training and evaluation.\n","\n","---\n","\n"],"metadata":{"id":"XfNL6Fdc7KxI"}},{"cell_type":"markdown","source":["# **A Practical Example.**\n","# Fine-Tunning  ViT on the CIFAR-10 Dataset.\n","\n","Vision Transformers (ViTs) apply transformer models to image data and have achieved state-of-the-art results in image classification tasks. They work by dividing images into patches, processing them like sequences of tokens, and using self-attention mechanisms to learn relationships between the patches.\n","\n","Let's intall the libraries:"],"metadata":{"id":"Oge-cnBd763t"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"wHIccd-Zqw9l","executionInfo":{"status":"ok","timestamp":1729621180726,"user_tz":-180,"elapsed":2587,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}}},"outputs":[],"source":["!pip install -q transformers datasets"]},{"cell_type":"markdown","source":["Import the required packages and check GPU availability"],"metadata":{"id":"tM3hiy30A4ep"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n","import torchvision.transforms as transforms\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# Check if a GPU is available\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xcR40FBAgST","executionInfo":{"status":"ok","timestamp":1729621180727,"user_tz":-180,"elapsed":22,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"aff05109-f6a4-4f42-8b92-a34478da0391"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Let's import our model! For this tutorial we will use the classic `ViT-base-patch-16`, the fenomenon model that revolutionized Computer Vision, with state of the art performance. This model pretrained in the huge `ImagNet-21` dataset from google."],"metadata":{"id":"jQSeMQm2ePuk"}},{"cell_type":"code","source":["model_name = \"google/vit-base-patch16-224-in21k\"\n","# Load the pre-trained ViT model for image classification\n","model = ViTForImageClassification.from_pretrained(\n","    model_name,                           # Pre-trained on ImageNet21k\n","    num_labels=102,                       # Oxford Flowers 102 has 102 classes\n",")\n","print(model.classifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ml58TOgmglGK","executionInfo":{"status":"ok","timestamp":1729621180729,"user_tz":-180,"elapsed":19,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"3686e033-e867-46aa-8a5b-37b90e14f26d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["Prepare the dataset"],"metadata":{"id":"uLrIZP92BEwe"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","import torchvision\n","import torch\n","from datasets import Dataset, DatasetDict\n","from transformers import ViTFeatureExtractor\n","from PIL import Image\n","\n","# Define some Hyperparameters\n","batch_size = 32\n","num_classes = 102\n","num_epochs = 30\n","\n","# Define some data Augmentations and transformations for the images\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224,224)),               # Resize the image to 224x224\n","    transforms.RandomResizedCrop(224),          # Randomly crop the image\n","    transforms.RandomHorizontalFlip(),          # Randomly flip the image to the horizontal axis\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly change the brightness, the contrast and hue\n","\n","    # Transform to Tensor and Normalize the values\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","eval_transforms = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Κανονικοποίηση\n","])\n","\n","# Load the datasets\n","train_dataset = torchvision.datasets.Flowers102(root='./data', split=\"train\", transform=train_transforms, download=True)\n","valid_dataset = torchvision.datasets.Flowers102(root='./data', split=\"val\", transform=eval_transforms, download=True)\n","test_dataset = torchvision.datasets.Flowers102(root='./data', split=\"test\", transform=eval_transforms, download=True)\n","\n","# Convert to Hugging Face Dataset\n","def create_huggingface_dataset(dataset):\n","    images = []\n","    labels = []\n","    for img, label in dataset:\n","        # Convert tensor back to PIL Image for compatibility with feature extractor\n","        img = transforms.ToPILImage()(img)\n","        images.append(img)\n","        labels.append(label)\n","    return Dataset.from_dict({\"image\": images, \"label\": labels})\n","\n","train_dataset_hf = create_huggingface_dataset(train_dataset)\n","valid_dataset_hf = create_huggingface_dataset(valid_dataset)"],"metadata":{"id":"-pZKzv8sBLkR","executionInfo":{"status":"ok","timestamp":1729621262300,"user_tz":-180,"elapsed":81585,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["Now, lets prepare the inputs to the model, for this we will use the `ViTFeatureExtractor`"],"metadata":{"id":"7PENySGYc_04"}},{"cell_type":"code","source":["from transformers import ViTImageProcessor\n","# Load the ViT Feature extract\n","feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","\n","# Prepare the data\n","# Define a preprocessing function to apply feature extractor\n","def preprocess_images(examples):\n","    # Apply the feature extractor for each image\n","    inputs = feature_extractor([image for image in examples['image']], return_tensors='pt')\n","    inputs['label'] = examples['label']\n","    return inputs\n","# Apply preprocessing function to train and validation sets\n","train_dataset_hf = train_dataset_hf.map(preprocess_images, batched=True)\n","valid_dataset_hf = valid_dataset_hf.map(preprocess_images, batched=True)\n","\n","# Set the format to PyTorch tensors for easier integration\n","train_dataset_hf.set_format(type='torch', columns=['pixel_values', 'label'])\n","valid_dataset_hf.set_format(type='torch', columns=['pixel_values', 'label'])\n","\n","# Create a DatasetDict for the trainer\n","flowers_hf_dataset = DatasetDict({\n","    \"train\": train_dataset_hf,\n","    \"validation\": valid_dataset_hf,\n","})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["d36825052cba4c829c91ca2cd25e3b95","9e10d696848e40fab4d2b2da486dbc0f","b049d886c8d74c638d3dc5a014738979","a816c30de54049968538b26f34647a27","3b95fca329454d9fb5c64027c11ead14","b817869598ef41bd9b5e7898aaa3dfd1","c4635ac7070e45f296407c79aba89ace","953132923cef4b80af41c3b947e12f31","48f32fff05e14880a1218ebcc75864a7","e5c30a94d50b44c2a71340fb882ca1a1","94bac8c5d8ed40df86614012a10ebaf4","424fecd0cbe24c8aab1f0acbf1fedabb","21da699d2cbe4175be34a0a9662d3dd6","912f1b0d2d804295a73631ea289d4dac","38a9c1655e144f15aa826b9b1a182d9a","366e035b7af744828df08f7c18e08105","eab82023b86a4388ad6405786346bd6b","f18d02d354254d2c88753c036c66a361","37ff5f9c9eb84b149a0c5de64b3f7b8b","99bb2441a92b40c682100dc40dba020a","30b844f160d849599d1b706ac80478d3","99a511ba608f4146aa36f24cfc101bbe"]},"id":"RPBKYrBCc8SP","executionInfo":{"status":"ok","timestamp":1729621382464,"user_tz":-180,"elapsed":120191,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"e0ddb1c5-d846-4eda-e3f1-1a832da7b8be"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1020 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36825052cba4c829c91ca2cd25e3b95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1020 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424fecd0cbe24c8aab1f0acbf1fedabb"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# Training arguments, including early stopping and other settings\n","training_args = TrainingArguments(\n","    output_dir=\"./results-oxFl102\",             # Output directory for model\n","    evaluation_strategy=\"epoch\",                # Evaluate at the end of each epoch\n","    learning_rate=2e-5,                         # Lower learning rate for fine-tuning\n","    per_device_train_batch_size=32,             # Batch size for training\n","    per_device_eval_batch_size=32,              # Batch size for evaluation\n","    num_train_epochs=5,                         # Number of epochs to train\n","    weight_decay=0.01,                          # Weight decay for regularization\n","    metric_for_best_model=\"accuracy\",           # Track the best model based on accuracy\n","    load_best_model_at_end=True,                # Load the best model after training\n","    save_strategy=\"epoch\",                      # Save model checkpoint at the end of each epoch\n","    logging_dir=\"./logs\",                       # Directory for logs\n","    logging_steps=10,                           # Log every 10 steps\n","    save_total_limit=2,                         # Save only the 2 best models\n",")\n","\n","# Add early stopping callback to stop training when no improvement is observed\n","early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience=2,   # Stop after 2 epochs with no improvement\n","    early_stopping_threshold=0.01 # Minimum improvement threshold\n",")\n","\n","\n","# Create a function that computes the accuracy score\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return dict(accuracy=accuracy_score(predictions, labels))\n","\n","\n","# Initialize the Trainer with the model, data, and training arguments\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=flowers_hf_dataset[\"train\"],\n","    eval_dataset=flowers_hf_dataset[\"validation\"],\n","    tokenizer=image_processor,               # ViT uses image processor as tokenizer\n","    compute_metrics=compute_metrics,         # Function to compute metrics\n","    callbacks=[early_stopping_callback]      # Add early stopping\n",")\n","\n","# Train the model (fine-tuning)\n","eval_results = trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZiS19f1MeK5q","executionInfo":{"status":"ok","timestamp":1729621382464,"user_tz":-180,"elapsed":17,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}},"outputId":"8243d974-d634-48f4-b330-00e7a5d15424"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Now, we are ready to train our model! The only thing that we must do is to pass all of this along with our datasets to the `Trainer`"],"metadata":{"id":"0y9m1DWSiwru"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":777},"id":"Tc_23-JSi_Zi","outputId":"d3dc5cc4-ce95-4687-b5b0-51e41e6a3c85","executionInfo":{"status":"ok","timestamp":1729622130163,"user_tz":-180,"elapsed":747710,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}}},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [160/160 12:21, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>4.601003</td>\n","      <td>0.022549</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>4.545810</td>\n","      <td>0.060784</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>4.499089</td>\n","      <td>0.083333</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>4.446427</td>\n","      <td>0.113725</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>4.404021</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>4.362044</td>\n","      <td>0.155882</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>4.333052</td>\n","      <td>0.179412</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>4.314417</td>\n","      <td>0.189216</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>No log</td>\n","      <td>4.289273</td>\n","      <td>0.203922</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>No log</td>\n","      <td>4.272733</td>\n","      <td>0.214706</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>No log</td>\n","      <td>4.264336</td>\n","      <td>0.223529</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>No log</td>\n","      <td>4.246622</td>\n","      <td>0.228431</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>No log</td>\n","      <td>4.240987</td>\n","      <td>0.228431</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>No log</td>\n","      <td>4.233804</td>\n","      <td>0.236275</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>No log</td>\n","      <td>4.224083</td>\n","      <td>0.241176</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>No log</td>\n","      <td>4.222130</td>\n","      <td>0.243137</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>No log</td>\n","      <td>4.218805</td>\n","      <td>0.238235</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>No log</td>\n","      <td>4.217607</td>\n","      <td>0.238235</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>No log</td>\n","      <td>4.216852</td>\n","      <td>0.241176</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>No log</td>\n","      <td>4.215192</td>\n","      <td>0.241176</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=160, training_loss=3.415279769897461, metrics={'train_runtime': 744.9336, 'train_samples_per_second': 27.385, 'train_steps_per_second': 0.215, 'total_flos': 1.5822534419693568e+18, 'train_loss': 3.415279769897461, 'epoch': 20.0})"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# Evaluate the model's performance after fine-tunning\n","eval_results = trainer.evaluate()\n","print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")"],"metadata":{"id":"q3nDeTkVc1hl","executionInfo":{"status":"ok","timestamp":1729622130164,"user_tz":-180,"elapsed":21,"user":{"displayName":"Despina Ioanna Chalkiadaki","userId":"18085928354633704515"}}},"execution_count":36,"outputs":[]}]}